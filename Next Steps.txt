- implement outline LFO (only volume of the original voice for now)
	# ig LFOs will have to go directly into the audio engine (not into the voices) so that there won't be any problems with polyphony and/or modulation later on
	# do reference the LFO in the voice though, but only advance its position while the voice isn't tailing off (-> no problems with polyphony later on)
	# render LFO waveform
	# implement changing the LFO's rate in the settings menu
	- make it possible to modulate other voices
		# change the RegionLfo's modulation function so that it also takes Voice* as an input
		# keep an array of voices in the LFO
			/ by default, only the associated region is in that list
		# overwrite the LFO's update function, so that it applies its update to all voices in the array, in the order they are contained in there
		# in the region's settings menu, show an area with a list of all the different regions (-> currenty equal to their voices, later on it will represent all the voices of the respective region)
		# make all items in that list checkable
		# when loading the menu, look up all the different regions (currently through the voices, later on through the voices' ids) and list them in the LFO's list
			# for a smoother workflow, it would be amazing if the items also took on their respective region's fillColour!
		# when checking/unchecking items from the list, update the LFO's voice array
			# fixed order: smallest region id to largest region id
		# bug: LFOs currently don't modulate their own associated region, only other regions
 !		- later: when deleting voices, make sure to adjust the LFOs' lists as well!
			- make the broadcaster a ChangeBroadcaster and add the settings menus as listeners -> notify when the LFOs list changes
	# create unipolar/bipolar LFO modes
		# unipolar: rewrites(!) the LFO's values to 0...1  ->  good for velocity modulation for instance
		# bipolar: rewrites(!) the LFO's values to -1...1  ->  good for pitch modulation for instance
	- implement other parameters to modulate
		!# there will most likely be one major problem with this: the voices' samples are probably not rendered in parallel; it's more likely that each voice renders 1 entire audio block in order
			(/) possible, but rather complicated fix: create an array of LFO modulation buffers in each voice. when an LFO updates another region's parameters, it instead writes an LFO modulation buffer (containing the values that it took on during the current block) and adds that block to the modulation buffer lists of every affected other voice as well as the parameter that the buffer should affect. while rendering, a voice will also parse the current sample of every buffer in its LFO modulation list and thus achieve the modulation sample by sample again. not sure how fast/slow this approach would be, though...
			# other idea: would it be possible for the synth to only render audio blocks one single sample at a time? that would also fix this problem (not sure if it would affect speed though)
				-> alright, apparently this works! will have to see whether this affects performance though. other than that, the only downside is that effects which take blocks to process cannot be used anymore (e.g. most filters, perhaps distortion?), but that should be fine, they probably aren't strictly needed or there might be workarounds
				?- as a workaround for block-based processing: maybe synth.renderNextBlock could only render the raw voice samples (incl. LFO adjustments), and afterwards there could be a method that triggers the block-based processing for all active voices. they only need to remember how their block-based effects' parameters were modulated during this block, though, which might further reduce performance...
		!# one more minor problem: how to handle parameters that are affected by multiple LFOs?
			(-) if every voice were to be handled sample by sample in parallel, it would probably be enough to change the respective setModulated[Parameter] methods to just multiply the modulated value by the passed amount, and to reset the modulated value to the base value at the beginning of each sample
				!# actually, making a variable that collects the multiplications (multiplier = 1.0 * mod1 * mod2 * ...) would be more consistent because parameter modifiers based on addition should be handled in the same manner (sum = 0.0 + mod1 + mod2 + ...) to avoid premature overflows (e.g. sum = 0.0 + 1.0 + 1.0 - 1.0 would return 0.0 when checking for overflows after every addition, but with all mods combined it would return 1.0)!
		# other voices' volume
			# also add a base volume slider to the settings menu (-60dB...+6dB)
		# voices' playback speed (incl. its own)
			# also add a base pitch slider to the settings menu (-60st...+60st -> since one octave doubles/halves the frequency, this corresponds to multiplication by 1/32...32, the formula being mult=2^(semis/12))
		# LFOs' rates (incl. its own)
		/ LFOs' modulation depths (incl. its own)
 !		- voices' playback positions (incl. its own)
			- would require adding a "slow update" mode to the LFO where it doesn't update during every single sample
			- also add a base start position slider to the settings menu (0.0 ... 100.0 ?)
	/ implement changing the LFO's modulation depth in the settings menu
 !		!- make it depend on the shape's size, actually
			- use the shape's area (bounds) in relation to that of the full image -> being >=2/3 of the image's area should result in full modulation depth
(# outline animations for the LFOs)
!# add a second column of toggle buttons to the LFO voice list that invert the modulation for that region
	-> this would be quite powerful since it enables call-and-response patterns (e.g. while one voice is loud another is quiet / while one voice is high another is low / ...)
	- this isn't as straightforward as it might seem since the modulation function is the same for all voices (whereas inversion would only be applied to some of the voices)!
		- solution: pass an Array<bool> to the lfo that says for which voices inversion should take place. when iterating over the voices, also iterate over that array and apply inversion on the go
			- is there a way to make this possible without if cases? for bipolar LFOs it would be enough to have an Array<int> with either +1 (not inverted) or -1 (inverted) and multiply the LFO value with one of these entries. but what about unipolar LFOs? inverting their values would be (1 - value), which can't really be put into an array
			-> use a second audio buffer containing the inverted waveform? this wouldn't save the if-case, but the following calculation...
		!# (probably) more efficient solution: instead of having just one modulation function, have one modulation function *per voice*
			#> at that point, it would also be easier to modulate different parameters of different voices (essentially making the list into *a mod matrix for every single region*), so that might actually be neat. but, to do that, the checkbox for the modulated parameter would have to also be moved into the listbox instead of being above it, which would require some more work
				# adjust CheckBoxList and CheckBoxListItem
				# Problem: how to handle Polarity? if there are different modulation functions, those are very likely to require different polarities...
					- naive solution: calculate the required polarity on the go -> too slow
					!# compromise: pre-calculate *both* polarities and select the one that's required... -> uses more memory, but saves multiplications
				# rework the modulationFunction/affectedVoices setup in RegionLfo
				# adjust the updateLfoParameter and copy Parameters methods in LfoEditor
	?/ each inversion button is only visible/enabled if the region has been selected
	# actually, an inversion button would be impractical - just make the inverted versions separate parameters
# (D)AHDSR envelope for voices
	# use this as a tester for a cleaner implementation of the state model maybe
	(-) if it's easy to do, also apply the same envelope to the region's LFO, not just the voice(s)
# split the RegionEditor into a different file
	-> possible now that i know how reference forwarding works
	-> create .h and .cpp for SegmentedRegion and for RegionEditor and put empty references to the other at the top of each header class
# split the LfoEditor into a different file
	-> possible now that i know how reference forwarding works
	-> create .h and .cpp for LfoEditor and for RegionEditor and put empty references to the other at the top of each header class
# DAHDSR: skip sustain state if sustain level is 0.0
	# would this cause problems in Voice? -> nope
# bipolar LFOs don't work as they intuitively should yet
	- currently: points closest to the focus point are at 0.0, points furthest away from the focus are either 1.0 or -1.0 (?)
	- how it should be: points closest to the focus point are -1.0, points furthest away are 1.0
	-> the bug was, that the reference point used wasn't actually the focus point *relative to the size of the region*, but actually the raw focus point - i.e. the focus point was assumed to always be in the top-left corner (between (0.0, 0.0) and (1.0, 1.0)...)
- improvements to processing speed
	-> implementing states in a cleaner fashion would probably help the most since it'd cut down on tons of if-cases
	# implement states for Lfo and/or(?) RegionLfo
	- get rid of std::function in RegionLfo
	- add a parameter "update time" (or so - just don't use frequency since it needs to be in s, not 1/s) to the Lfo class -> adjustable by the user, 0.0s (real-time updates) allowed
	- implement states for Voice
	- maybe switch to function templates instead of std::function -> see https://stackoverflow.com/questions/25848690/should-i-use-stdfunction-or-a-function-pointer-in-c
		https://vittorioromeo.info/index/blog/passing_functions_to_functions.html
		https://stackoverflow.com/questions/43948240/sfinae-failing-when-evaluating-a-constexpr-in-a-template-parameter/43949301?noredirect=1#comment74933533_43949301
		https://stackoverflow.com/questions/28746744/passing-capturing-lambda-as-function-pointer/28746827#28746827
		https://stackoverflow.com/questions/9054774/difference-between-stdfunction-and-a-standard-function-pointer/9054802#9054802
	- make use of certain keywords
		- noexcept: tells the compiler that the method won't throw any exceptions -> more optimisations possible
		- inline / __forceinline: tell the compiler that the method should be inlined
			- methods that are defined in the header of a file are implicitly inline; if they are defined in the cpp, inline can be added at the beginning. forceinline should be used sparingly as too much inlining may actually hurt performance due to larger binaries and paging
		- final (classes)
			- for the state implementations
			- see https://devblogs.microsoft.com/cppblog/the-performance-benefits-of-final-classes/
		/ register keyword?
			- probably overkill, the compiler should do this automatically already...
		- when to use references and when not?
	- correctly using const pointers (vs. pointer consts and const pointer consts) might also be useful for making things cleaner (though not necessarily faster)
		- see https://stackoverflow.com/questions/1143262/what-is-the-difference-between-const-int-const-int-const-and-int-const
		- for functions, see also https://stackoverflow.com/questions/751681/meaning-of-const-last-in-a-function-declaration-of-a-class
	- correctly using references could also improve some things (e.g. using references instead of pointers for state implementations and regions' voices, LFOs etc. might be worth thinking about)
		- https://www.tutorialspoint.com/cplusplus/cpp_references.htm
		- note also: https://stackoverflow.com/questions/51705967/advantages-of-pass-by-value-and-stdmove-over-pass-by-reference
		- first comment of the first reply here is also important: https://stackoverflow.com/questions/21215409/does-c-pass-objects-by-value-or-reference
			- for a more direct example: https://stackoverflow.com/questions/9293674/can-we-reassign-the-reference-in-c#9293732
	- declare methods which don't change member variables const: https://stackoverflow.com/questions/24500909/passing-class-reference-to-function
- bug: the LFO line doesn't keep updating while the voice is tailing off (-> just create a bool variable isPlaying in Voice that's checked for that purpose)
- is there a way to make it so that modulation of other regions remains at the current modulation value even when the region modulating it stops playing?
	-> example: currently, if region 1 modulates region 2's volume, the modulation stops after region 1 stops playing (i.e. region 2 plays with its base volume - not the base volume multiplied with the LFO value of region 1 that was last measured). is there a way to apply the LFO's modulation independent from a region playing?
	- idea: maybe advance all LFOs *before* rendering all next synth samples? would that change anything/much?
		!- this would also have the advantage that it wouldn't require a Voice (i.e. an audio file) for a region's LFO to work
		- problem: this would require either the audio engine or the LFOs to keep track of which LFOs are playing, which could require additional if-cases every sample...
			!- solution part 1: when implementing the DAHDSR envelope for the LFOs, it would require them to keep track of whether or not they are playing anyway -> two birds with one stone!
			!- solution part 2: after implementing states for the LFOs and the DAHDSR, there might as well be "Playing" and "Not Playing" states, converting the additional if cases to quick array accesses
			-> definitely doable! now the only remaining question is whether evaluating the LFOs before rendering the voices changes the sound too much
	- idea 2: maybe don't use a single variable to accumulate modulations. instad, use a list of modulation values with are multiplied/summed during every sample. when an LFO starts modulating a value, its value is inserted into the list and updated after every sample - but only removed when the LFO is deleted or starts modulating a different value. downside: more operations during every sample
(- implement more LFO-modulatable parameters)
- update LFO menu (might not be necessary anymore depending on what has been done at the point that this is next)
- make it possible to delete regions
	- to handle all the audio parameters, create a method in the *audio engine*(!) that deletes all voices with the region's id from the LFOs' voice lists, and then all of those voices from the synth
- serialisation




- improve drawing algorithm using image information
	- differentiate between left and right clicks: left clicks use the automated outline finder, right clicks simply draw straight lines like they do now -> more flexible
- fix some bugs/things that would cause problems with unknowing users
	- since regions currently only keep their voice's index, errors would occur if regions that were created earlier than that region were deleted
		-> fix: when implementing polyphony, voices will have a certain variable that refers to their region. simply delete only voices with the corresponding variable value -> two birds with one stone
	- voices would currently keep playing while toggled on and switching to editing -> region could be deleted while playing -> errors in audio buffer!
		-> fix: tell the synth to stop all notes without tailoff when switching from playing to any other state
		- actually, it's kiiiinda nice to be able to edit an LFO's parameters while the sound is still playing... maybe there's a way to only force a stop when selecting a new audio file?
	- when setting a region to togglable, they also toggle in editing mode
		-> fix: only update their toggleable state when switching to playing
		- see above bug; maybe make an exception for editing mode (but switch all regions to off when switching back to playing mode maybe)
	- make it so that regions can only be created when at least 3 points have been added to the screen (might already be in the code)
		(!)- bonus: make it so that regions can only be created if the points aren't on a line
	- safer audio file loading (see juce tutorial)
(- panning setting for each region)
- implement states the cleaner way: using the actual state model (i.e. using interfaces)
	- for PluginEditor
	- for SegmentableImage
	- for SegmentedRegion
	- for Lfo (updating the parameter while wavetable isn't set currently takes up ressources every sample)
	- for Voice/Sound probably (again, ressources are taken up while the audio data hasn't been loaded yet)
!- allow for 2-voice-polyphony
	- add *2* voices *and 1 sound* per region
	- add a variable to the voice and oscillator class that represents the region, and tell it that it can only play sounds if its variable corresponds to that of the oscillator
		- add a counter variable (uint should suffice) to the audio engine. this variable is incremented every time a new region is created. the regions take on the current value of the counter when they are created. -> unique ID
		- display the region's ID in the title of the region's settings menu!
		- all juce::Component objects should have a methods getComponentID/setComponentID -> use those
	- instead of using Voice.startNote, use synth.noteOn to play sounds
		- maybe overwrite this somehow? how will the synth know which sound to play?
			!- maybe make it so the sound can only play one specific MIDI note, and the region plays that specific note
				- this has the added benefit that implementing MIDI support for the instrument will be easier - just make that MIDI note adjustable in the region editor!
- add some kind of information button that displays key mappings for each mode (e.g. using backspace to delete points while in drawing mode)
(- add playback modes for the voices: Normal (Forward), Oneshot, Forward-Backward, Backward-Forward, Backward)
!- add an option to quantise the start/stop timings of voices to certain note values corresponding to the DAW's BPM
	- this would be an amazing quality of life change, but idk how easy this would be... i know that the DAW's BPM should be contained in some MIDI signals, but idk where or when or how to evaluate them to achieve synchronisation
- set help texts for all components
	- there should already be a method setHelpText for all Component members
- option to set an LFO's rate to that of its associated region's sound
- perhaps modulate a region's LFO depth with its AHDSR modulator (whether or not it's possible to modulate LFO depth otherwise)
- add Parameters for the DAW
